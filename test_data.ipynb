{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1edfc75",
   "metadata": {},
   "source": [
    "# SFT Data Verification\n",
    "\n",
    "This notebook loads and examines the SFT training data to verify the prompt and label structure used in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3a778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/qfitterey3/scratch/envs/llada/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_from_disk, DatasetDict, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import sys\n",
    "\n",
    "# Add the path to import the SFT training modules\n",
    "sys.path.append('/home/hice1/qfitterey3/scratch/LLada-Reasoning/sft_training')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb4b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTDataset class defined\n"
     ]
    }
   ],
   "source": [
    "# Recreate the SFTDataset class from the training script\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.dataset[idx]\n",
    "        user_ids = self.tokenizer.encode(ex['user'] + self.tokenizer.eos_token, add_special_tokens=False)\n",
    "        asm_ids = self.tokenizer.encode(ex['assistant'] + self.tokenizer.eos_token, add_special_tokens=False)\n",
    "        prompt_length = len(user_ids)\n",
    "        input_ids = (user_ids + asm_ids)[:self.max_length]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long), \n",
    "            \"prompt_length\": prompt_length,\n",
    "            \"raw_user\": ex['user'],\n",
    "            \"raw_assistant\": ex['assistant']\n",
    "        }\n",
    "\n",
    "print(\"SFTDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25c1e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.dataset[idx]\n",
    "        user_ids = self.tokenizer.encode(ex['user'] + self.tokenizer.eos_token, add_special_tokens=False)\n",
    "        asm_ids = self.tokenizer.encode(ex['assistant'] + self.tokenizer.eos_token, add_special_tokens=False)\n",
    "        prompt_length = len(user_ids)\n",
    "        input_ids = (user_ids + asm_ids)[:self.max_length]\n",
    "        return {\"input_ids\": torch.tensor(input_ids, dtype=torch.long), \"prompt_length\": prompt_length}\n",
    "\n",
    "# -------- SFT Data Collator --------\n",
    "class SFTDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids_list = [ex['input_ids'] for ex in features]\n",
    "        prompt_lengths = torch.tensor([ex['prompt_length'] for ex in features], dtype=torch.long)\n",
    "        padded_input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.pad_id)\n",
    "        return {'input_ids': padded_input_ids, 'prompt_lengths': prompt_lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e7f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"./llada_local\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "special_tokens_dict = {}\n",
    "if tokenizer.pad_token is None: special_tokens_dict[\"pad_token\"] = \"<|pad|>\"\n",
    "if tokenizer.convert_tokens_to_ids(\"<|mdm_mask|>\") == tokenizer.unk_token_id: special_tokens_dict[\"additional_special_tokens\"] = [\"<|mdm_mask|>\"]\n",
    "if special_tokens_dict:\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "mask_id, pad_id = tokenizer.convert_tokens_to_ids(\"<|mdm_mask|>\"), tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23d51b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llama-Nemotron-Post-Training-Dataset_train', 'Bespoke-Stratos-17k_train', 'OpenThoughts-114k_train', 'databricks-dolly-15k_train', 'SlimOrca_train']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk, DatasetDict, concatenate_datasets, interleave_datasets\n",
    "\n",
    "ds = load_from_disk(\"./sft_data/combined/\")  # Adjust the path to your dataset\n",
    "train_keys = [k for k in ds.keys() if k.endswith(\"_train\")]\n",
    "val_keys = [k for k in ds.keys() if k.endswith(\"_validation\")]\n",
    "print(train_keys)\n",
    "local_batch_size= 8\n",
    "train_datasets = [ds[k] for k in train_keys]\n",
    "interleaved_train = interleave_datasets(train_datasets, probabilities=[0.25,0.25,0.25,0.25], seed=42)\n",
    "train_ds = SFTDataset(interleaved_train, tokenizer, 8192)\n",
    "val_ds = SFTDataset(concatenate_datasets([ds[k] for k in ds if k.endswith(\"_validation\")]), tokenizer, 8192)\n",
    "collator = SFTDataCollator(tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=local_batch_size, collate_fn=collator)\n",
    "val_loader = DataLoader(val_ds, batch_size=local_batch_size, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14bd6bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created successfully\n",
      "Full Input 0: conversations: [{'from': 'system', 'value': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.', 'weight': None}, {'from': 'human', 'value': '2 years…a reality confirmed by a recent ct scan. thank you for all you do.\\n\\nHow would the previous sentence be correctly capitalized?', 'weight': 0.0}, {'from': 'gpt', 'value': '\"2 years... A reality confirmed by a recent CT scan. Thank you for all you do.\"\\n\\nIn this sentence, we made sure to capitalize the first letter of each sentence, like \"A\" in \"A reality\" and \"T\" in \"Thank you.\" We also capitalized both \"C\" and \"T\" in \"CT scan\" because it is a short form of \"Computed Tomography\" and the letters represent the first letter of each word.', 'weight': 1.0}]\n",
      "Prompt Length 0: 204\n",
      "Label 0: \n",
      "Prompt 0: conversations: [{'from': 'system', 'value': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.', 'weight': None}, {'from': 'human', 'value': '2 years…a reality confirmed by a recent ct scan. thank you for all you do.\\n\\nHow would the previous sentence be correctly capitalized?', 'weight': 0.0}, {'from': 'gpt', 'value': '\"2 years... A reality confirmed by a recent CT scan. Thank you for all you do.\"\\n\\nIn this sentence, we made sure to capitalize the first letter of each sentence, like \"A\" in \"A reality\" and \"T\" in \"Thank you.\" We also capitalized both \"C\" and \"T\" in \"CT scan\" because it is a short form of \"Computed Tomography\" and the letters represent the first letter of each word.', 'weight': 1.0}]\n",
      "--------------------------------------------------------------------------------\n",
      "Full Input 1: conversations: [{'from': 'system', 'value': 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.', 'weight': None}, {'from': 'human', 'value': 'Concepts: beach, boat, foreground\\n\\nWrite a sentence that includes all these words.', 'weight': 0.0}, {'from': 'gpt', 'value': 'As the sun descended towards the horizon, casting a warm glow on the serene beach, a rustic wooden boat elegantly floated into the foreground, captivating the onlookers with its natural charm and evoking a feeling of tranquility in the picturesque scene.', 'weight': 1.0}]\n",
      "Prompt Length 1: 147\n",
      "Label 1: \n",
      "Prompt 1: conversations: [{'from': 'system', 'value': 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.', 'weight': None}, {'from': 'human', 'value': 'Concepts: beach, boat, foreground\\n\\nWrite a sentence that includes all these words.', 'weight': 0.0}, {'from': 'gpt', 'value': 'As the sun descended towards the horizon, casting a warm glow on the serene beach, a rustic wooden boat elegantly floated into the foreground, captivating the onlookers with its natural charm and evoking a feeling of tranquility in the picturesque scene.', 'weight': 1.0}]\n",
      "--------------------------------------------------------------------------------\n",
      "Full Input 2: conversations: [{'from': 'system', 'value': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.', 'weight': None}, {'from': 'human', 'value': 'Can you tell me the answer to who is credited with performing the first heavier-than-air flight??', 'weight': 0.0}, {'from': 'gpt', 'value': 'Sure, little buddy! The Wright brothers, Orville and Wilbur, are credited with performing the first heavier-than-air flight. They were two smart inventors who built an airplane called the Wright Flyer. On December 17, 1903, they successfully flew their airplane for the first time in Kitty Hawk, North Carolina. It was a big day because it showed people that flying was possible!', 'weight': 1.0}]\n",
      "Prompt Length 2: 182\n",
      "Label 2: \n",
      "Prompt 2: conversations: [{'from': 'system', 'value': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.', 'weight': None}, {'from': 'human', 'value': 'Can you tell me the answer to who is credited with performing the first heavier-than-air flight??', 'weight': 0.0}, {'from': 'gpt', 'value': 'Sure, little buddy! The Wright brothers, Orville and Wilbur, are credited with performing the first heavier-than-air flight. They were two smart inventors who built an airplane called the Wright Flyer. On December 17, 1903, they successfully flew their airplane for the first time in Kitty Hawk, North Carolina. It was a big day because it showed people that flying was possible!', 'weight': 1.0}]\n",
      "--------------------------------------------------------------------------------\n",
      "Full Input 3: conversations: [{'from': 'system', 'value': 'You are an AI assistant that follows instruction extremely well. Help as much as you can.', 'weight': None}, {'from': 'human', 'value': 'Answer the following question: Please answer the following question about this movie plot. If it\\'s un-answerable, please output \"No answer\".  Question: Who plays Dr. Edward Morbius? Movie plot title: Forbidden Planet Movie plot: Opening sceneIt is late in the 22nd Century. United Planet cruiser C57D is a year out from Earth base on the way to Altair for a special mission. Commander J.J Adams (Leslie Neilsen) orders the crew to the deceleration booths as the ship drops from light speed to normal space.Adams orders pilot Jerry Farman (Jack Kelly) to lay in a course for the fourth planet. The captain then briefs the crew that they are at their destination, and that they are to look for survivors from the Bellerophon expedition 20 years earlier.As they orbit the planet looking for signs of life, the ship is scanned by a radar facility some 20 square miles in area. Morbius (Walter Pigeon) contacts the ship from the planet asking why the ship is here. Morbius goes on to explain he requires nothing, no rescue is required and he can\\'t guarantee the safety of the ship or its crew.Adams confirms that Morbius was a member of the original crew, but is puzzled at the cryptic warning Morbius realizes the ship is going to land regardless, and gives the pilot coordinates in a desert region of the planet. The ship lands and security details deploy. Within minutes a high speed dust cloud approaches the ship. Adams realizes it is a vehicle, and as it arrives the driver is discovered to be a robot (Robby). Robby welcomes the crew to Altair 4 and invites members of the crew to Morbius\\' residence.Adams, Farman and Doc Ostrow (Warren Stevens) arrive at the residence and are greeted by Morbius. They sit down to a meal prepared by Robby\\'s food synthesizer and Morbius shows the visitors Robby\\'s other abilities, including his unwavering obedience. Morbius then gives Robby a blaster with orders to shoot Adams. Robby refuses and goes into a mechanical mind lock, disabling him until the order is changed.Morbius then shows the men the defense system of the house (A series of steel shutters). When questioned, Morbius admits that the Belleraphon crew is dead, Morbius and his wife being the only original survivors. Morbius\\'s wife has also died, but months after the...\\nAnswer:', 'weight': 0.0}, {'from': 'gpt', 'value': 'Walter Pigeon plays Dr. Edward Morbius.', 'weight': 1.0}]\n",
      "Prompt Length 3: 607\n",
      "Label 3: \n",
      "Prompt 3: conversations: [{'from': 'system', 'value': 'You are an AI assistant that follows instruction extremely well. Help as much as you can.', 'weight': None}, {'from': 'human', 'value': 'Answer the following question: Please answer the following question about this movie plot. If it\\'s un-answerable, please output \"No answer\".  Question: Who plays Dr. Edward Morbius? Movie plot title: Forbidden Planet Movie plot: Opening sceneIt is late in the 22nd Century. United Planet cruiser C57D is a year out from Earth base on the way to Altair for a special mission. Commander J.J Adams (Leslie Neilsen) orders the crew to the deceleration booths as the ship drops from light speed to normal space.Adams orders pilot Jerry Farman (Jack Kelly) to lay in a course for the fourth planet. The captain then briefs the crew that they are at their destination, and that they are to look for survivors from the Bellerophon expedition 20 years earlier.As they orbit the planet looking for signs of life, the ship is scanned by a radar facility some 20 square miles in area. Morbius (Walter Pigeon) contacts the ship from the planet asking why the ship is here. Morbius goes on to explain he requires nothing, no rescue is required and he can\\'t guarantee the safety of the ship or its crew.Adams confirms that Morbius was a member of the original crew, but is puzzled at the cryptic warning Morbius realizes the ship is going to land regardless, and gives the pilot coordinates in a desert region of the planet. The ship lands and security details deploy. Within minutes a high speed dust cloud approaches the ship. Adams realizes it is a vehicle, and as it arrives the driver is discovered to be a robot (Robby). Robby welcomes the crew to Altair 4 and invites members of the crew to Morbius\\' residence.Adams, Farman and Doc Ostrow (Warren Stevens) arrive at the residence and are greeted by Morbius. They sit down to a meal prepared by Robby\\'s food synthesizer and Morbius shows the visitors Robby\\'s other abilities, including his unwavering obedience. Morbius then gives Robby a blaster with orders to shoot Adams. Robby refuses and goes into a mechanical mind lock, disabling him until the order is changed.Morbius then shows the men the defense system of the house (A series of steel shutters). When questioned, Morbius admits that the Belleraphon crew is dead, Morbius and his wife being the only original survivors. Morbius\\'s wife has also died, but months after the...\\nAnswer:', 'weight': 0.0}, {'from': 'gpt', 'value': 'Walter Pigeon plays Dr. Edward Morbius.', 'weight': 1.0}]\n",
      "--------------------------------------------------------------------------------\n",
      "Full Input 4: conversations: [{'from': 'system', 'value': 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.', 'weight': None}, {'from': 'human', 'value': 'At the time she wrote the Longshot miniseries, writer Ann Nocenti was pursuing her Master\\'s degree at the School of International and Public Affairs, Columbia University, working at the magazine Lies of Our Times, and reading the work of writers like Marshall McLuhan, Noam Chomsky, Edward S. Herman and Walter Lippmann. Mojo, a slaver and dictator who rules his dimension through the television programs he produces, was created as a direct result of these influences. (A character named Manufactured Consent after the Chomsky book of the same name, who appeared in Nocenti\\'s 1990 book The New Mutants Summer Special, was also born of these works.) Artist Art Adams designed the character per Nocenti\\'s instructions that he be disgusting and unpleasant, and also tried to make him look frightening. The wires that hold Mojo\\'s eyelids open, thus preventing him from blinking, were inspired by an interview with actor Malcolm McDowell on Late Night with David Letterman, in which McDowell revealed that the similar apparatus he had to wear for the Ludovico technique scene in A Clockwork Orange had scarred his corneas. The rest of the equipment attached to Mojo\\'s head controls his mechanized chair.  Mojo first appeared in Longshot #4 (December 1985), and was the main villain of the miniseries, appearing in the subsequent three issues.  The character subsequently appeared in The New Mutants Annual #2 (1986). That same year, in The Uncanny X-Men Annual #10, Mojo appeared as the villain in the story, which saw Longshot join the X-Men. In 1988, Mojo appeared in a backup story in The Uncanny X-Men Annual #12. He was the main villain of the one-shot special Excalibur: Mojo Mayhem (December 1989). Subsequent appearances include Marvel Comics Presents #89 (1991), The Uncanny X-Men Annual #15 (1991), Wolverine #52 (March 1992), X-Men #6 (March 1992), Wolverine #53 (April 1992), X-Men #7 (April 1992), #10-11 (July-August 1992), The Uncanny X-Men Annual 16 (Annual 1992), Marvel Comics Presents #119 (1993), What If? #59 (March 1994), X-Men Adventures: Season Two #11 (December 1994), Marvel: Portraits of the Universe #1 (March 1995), X-Men: Mutations #1 (1996), Youngblood/X-Force #1 (July 1996), X-Force/Youngblood #1 (August 1996), X-Force #60-61 (November-December 1996), The Adventures of the X-Men #9-10 (1996-January 1997), X-Babies: Murderama #1 (January 1998), X-Force #76 (April 1998), X-Babies Reborn #1 (January 2000), X-Men Unlimited #32 (September 2001), Exiles #18-19 (December 2002-January 2003), The Uncanny X-Men #460-461 (August 2005), and Exiles #73-74 (February 2006).  Mojo was featured in an entry in the Official Handbook of the Marvel Universe Deluxe Edition #8, and in the All-New Official Handbook of the Marvel Universe A-Z #7 (2006).\\n\\nAnswer this question \"Who published this comic book featuring Mojo?\" by extracting the answer from the text above.', 'weight': 0.0}, {'from': 'gpt', 'value': 'Marvel Comics published the comic book featuring Mojo.', 'weight': 1.0}]\n",
      "Prompt Length 4: 939\n",
      "Label 4: \n",
      "Prompt 4: conversations: [{'from': 'system', 'value': 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.', 'weight': None}, {'from': 'human', 'value': 'At the time she wrote the Longshot miniseries, writer Ann Nocenti was pursuing her Master\\'s degree at the School of International and Public Affairs, Columbia University, working at the magazine Lies of Our Times, and reading the work of writers like Marshall McLuhan, Noam Chomsky, Edward S. Herman and Walter Lippmann. Mojo, a slaver and dictator who rules his dimension through the television programs he produces, was created as a direct result of these influences. (A character named Manufactured Consent after the Chomsky book of the same name, who appeared in Nocenti\\'s 1990 book The New Mutants Summer Special, was also born of these works.) Artist Art Adams designed the character per Nocenti\\'s instructions that he be disgusting and unpleasant, and also tried to make him look frightening. The wires that hold Mojo\\'s eyelids open, thus preventing him from blinking, were inspired by an interview with actor Malcolm McDowell on Late Night with David Letterman, in which McDowell revealed that the similar apparatus he had to wear for the Ludovico technique scene in A Clockwork Orange had scarred his corneas. The rest of the equipment attached to Mojo\\'s head controls his mechanized chair.  Mojo first appeared in Longshot #4 (December 1985), and was the main villain of the miniseries, appearing in the subsequent three issues.  The character subsequently appeared in The New Mutants Annual #2 (1986). That same year, in The Uncanny X-Men Annual #10, Mojo appeared as the villain in the story, which saw Longshot join the X-Men. In 1988, Mojo appeared in a backup story in The Uncanny X-Men Annual #12. He was the main villain of the one-shot special Excalibur: Mojo Mayhem (December 1989). Subsequent appearances include Marvel Comics Presents #89 (1991), The Uncanny X-Men Annual #15 (1991), Wolverine #52 (March 1992), X-Men #6 (March 1992), Wolverine #53 (April 1992), X-Men #7 (April 1992), #10-11 (July-August 1992), The Uncanny X-Men Annual 16 (Annual 1992), Marvel Comics Presents #119 (1993), What If? #59 (March 1994), X-Men Adventures: Season Two #11 (December 1994), Marvel: Portraits of the Universe #1 (March 1995), X-Men: Mutations #1 (1996), Youngblood/X-Force #1 (July 1996), X-Force/Youngblood #1 (August 1996), X-Force #60-61 (November-December 1996), The Adventures of the X-Men #9-10 (1996-January 1997), X-Babies: Murderama #1 (January 1998), X-Force #76 (April 1998), X-Babies Reborn #1 (January 2000), X-Men Unlimited #32 (September 2001), Exiles #18-19 (December 2002-January 2003), The Uncanny X-Men #460-461 (August 2005), and Exiles #73-74 (February 2006).  Mojo was featured in an entry in the Official Handbook of the Marvel Universe Deluxe Edition #8, and in the All-New Official Handbook of the Marvel Universe A-Z #7 (2006).\\n\\nAnswer this question \"Who published this comic book featuring Mojo?\" by extracting the answer from the text above.', 'weight': 0.0}, {'from': 'gpt', 'value': 'Marvel Comics published the comic book featuring Mojo.', 'weight': 1.0}]\n",
      "--------------------------------------------------------------------------------\n",
      "Full Input 5: conversations: [{'from': 'system', 'value': 'You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.', 'weight': None}, {'from': 'human', 'value': 'Premise: \"A woman in a cowgirl hat is kneeling on a horse\\'s saddle while holding her hand over her heart.\"\\nHypothesis: \"The woman can ride a horse.\"\\nDo we know that the hypothesis entailed by the premise? Let\\'s be accurate as possible and think first.', 'weight': 0.0}, {'from': 'gpt', 'value': 'To determine if the hypothesis \"The woman can ride a horse\" is entailed by the premise, let\\'s analyze the premise step by step.\\n\\nPremise: \"A woman in a cowgirl hat is kneeling on a horse\\'s saddle while holding her hand over her heart.\"\\n\\n1. The woman is wearing a cowgirl hat.\\n2. The woman is kneeling on a horse\\'s saddle.\\n3. The woman is holding her hand over her heart.\\n\\nFrom the given information, we can infer the following:\\n\\n1. The woman is familiar with cowgirl attire and possibly the culture associated with it.\\n2. The woman is on a horse and in a position where she is interacting with the horse (kneeling on the saddle).\\n\\nAlthough the premise describes the woman in a specific position on a horse (kneeling on the saddle), it does not explicitly state that she can ride the horse in the traditional sense. However, it can be reasonably assumed that the woman has some level of familiarity or experience with horses, given that she is in a cowgirl hat and is on the horse\\'s saddle.\\n\\nTherefore, while the hypothesis \"The woman can ride a horse\" is not directly entailed by the premise, it is a reasonable assumption based on the information provided.', 'weight': 1.0}]\n",
      "Prompt Length 5: 431\n",
      "Label 5: \n",
      "Prompt 5: conversations: [{'from': 'system', 'value': 'You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.', 'weight': None}, {'from': 'human', 'value': 'Premise: \"A woman in a cowgirl hat is kneeling on a horse\\'s saddle while holding her hand over her heart.\"\\nHypothesis: \"The woman can ride a horse.\"\\nDo we know that the hypothesis entailed by the premise? Let\\'s be accurate as possible and think first.', 'weight': 0.0}, {'from': 'gpt', 'value': 'To determine if the hypothesis \"The woman can ride a horse\" is entailed by the premise, let\\'s analyze the premise step by step.\\n\\nPremise: \"A woman in a cowgirl hat is kneeling on a horse\\'s saddle while holding her hand over her heart.\"\\n\\n1. The woman is wearing a cowgirl hat.\\n2. The woman is kneeling on a horse\\'s saddle.\\n3. The woman is holding her hand over her heart.\\n\\nFrom the given information, we can infer the following:\\n\\n1. The woman is familiar with cowgirl attire and possibly the culture associated with it.\\n2. The woman is on a horse and in a position where she is interacting with the horse (kneeling on the saddle).\\n\\nAlthough the premise describes the woman in a specific position on a horse (kneeling on the saddle), it does not explicitly state that she can ride the horse in the traditional sense. However, it can be reasonably assumed that the woman has some level of familiarity or experience with horses, given that she is in a cowgirl hat and is on the horse\\'s saddle.\\n\\nTherefore, while the hypothesis \"The woman can ride a horse\" is not directly entailed by the premise, it is a reasonable assumption based on the information provided.', 'weight': 1.0}]\n",
      "--------------------------------------------------------------------------------\n",
      "Full Input 6: conversations: [{'from': 'system', 'value': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.', 'weight': None}, {'from': 'human', 'value': 'Here is some data: Albert B. White, spouse, Agnes Ward White; Albert B. White, birthPlace, Cleveland; Albert B. White, deathPlace, Parkersburg, West Virginia; Albert B. White, successor, William M. O. Dawson; William M. O. Dawson, birthPlace, Bloomington, Maryland.\\n\\nWrite a sentence that describes this data.\\nSentence:', 'weight': 0.0}, {'from': 'gpt', 'value': 'Albert B. White was a man who was born in a place called Cleveland, and he was married to a lady named Agnes Ward White. When he was older, he lived in a place called Parkersburg in West Virginia until he passed away. After that, another man named William M. O. Dawson, who was born in Bloomington in Maryland, took his place.', 'weight': 1.0}]\n",
      "Prompt Length 6: 236\n",
      "Label 6: \n",
      "Prompt 6: conversations: [{'from': 'system', 'value': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.', 'weight': None}, {'from': 'human', 'value': 'Here is some data: Albert B. White, spouse, Agnes Ward White; Albert B. White, birthPlace, Cleveland; Albert B. White, deathPlace, Parkersburg, West Virginia; Albert B. White, successor, William M. O. Dawson; William M. O. Dawson, birthPlace, Bloomington, Maryland.\\n\\nWrite a sentence that describes this data.\\nSentence:', 'weight': 0.0}, {'from': 'gpt', 'value': 'Albert B. White was a man who was born in a place called Cleveland, and he was married to a lady named Agnes Ward White. When he was older, he lived in a place called Parkersburg in West Virginia until he passed away. After that, another man named William M. O. Dawson, who was born in Bloomington in Maryland, took his place.', 'weight': 1.0}]\n",
      "--------------------------------------------------------------------------------\n",
      "Full Input 7: conversations: [{'from': 'system', 'value': 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.', 'weight': None}, {'from': 'human', 'value': 'Given the following passage  \"The teachings on the Four Noble Truths are regarded as central to the teachings of Buddhism, and are said to provide a conceptual framework for Buddhist thought. These four truths explain the nature of dukkha (suffering, anxiety, unsatisfactoriness), its causes, and how it can be overcome. The four truths are:[note 4]\",  answer the following question. Note that the answer is present within the text.  Question: Who might try to over come suffering, anxiety, and unsatisfactoriness?\\nA:', 'weight': 0.0}, {'from': 'gpt', 'value': 'A: Buddhists might try to overcome suffering, anxiety, and unsatisfactoriness, as the Four Noble Truths are central to the teachings of Buddhism and provide a conceptual framework for Buddhist thought that explains the nature of dukkha along with its causes and how it can be overcome.', 'weight': 1.0}]\n",
      "Prompt Length 7: 252\n",
      "Label 7: \n",
      "Prompt 7: conversations: [{'from': 'system', 'value': 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.', 'weight': None}, {'from': 'human', 'value': 'Given the following passage  \"The teachings on the Four Noble Truths are regarded as central to the teachings of Buddhism, and are said to provide a conceptual framework for Buddhist thought. These four truths explain the nature of dukkha (suffering, anxiety, unsatisfactoriness), its causes, and how it can be overcome. The four truths are:[note 4]\",  answer the following question. Note that the answer is present within the text.  Question: Who might try to over come suffering, anxiety, and unsatisfactoriness?\\nA:', 'weight': 0.0}, {'from': 'gpt', 'value': 'A: Buddhists might try to overcome suffering, anxiety, and unsatisfactoriness, as the Four Noble Truths are central to the teachings of Buddhism and provide a conceptual framework for Buddhist thought that explains the nature of dukkha along with its causes and how it can be overcome.', 'weight': 1.0}]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "print(\"Data loaders created successfully\")\n",
    "# please decode input and label tensors to check if they are correct\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    prompt_lengths = batch['prompt_lengths']\n",
    "    for i in range(len(input_ids)):\n",
    "        # Decode the full input as a single string\n",
    "        full_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "        print(f\"Full Input {i}: {full_text}\")\n",
    "        print(f\"Prompt Length {i}: {prompt_lengths[i]}\")\n",
    "        \n",
    "        # Decode just the label part (after prompt_lengths)\n",
    "        label_tokens = input_ids[i][prompt_lengths[i]:]\n",
    "        # Remove padding tokens\n",
    "        label_tokens = label_tokens[label_tokens != tokenizer.pad_token_id]\n",
    "        label_text = tokenizer.decode(label_tokens, skip_special_tokens=True)\n",
    "        print(f\"Label {i}: {label_text}\")\n",
    "        \n",
    "        # Also show the prompt part for clarity\n",
    "        prompt_tokens = input_ids[i][:prompt_lengths[i]]\n",
    "        prompt_text = tokenizer.decode(prompt_tokens, skip_special_tokens=True)\n",
    "        print(f\"Prompt {i}: {prompt_text}\")\n",
    "        print(\"-\" * 80)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3d36bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CALCULATING PROBABILITIES BASED ON DATASET SIZES ===\n",
      "Llama-Nemotron-Post-Training-Dataset_train: 500,000 samples\n",
      "Bespoke-Stratos-17k_train: 10,345 samples\n",
      "OpenThoughts-114k_train: 59,988 samples\n",
      "databricks-dolly-15k_train: 12,007 samples\n",
      "SlimOrca_train: 414,339 samples\n",
      "\n",
      "Total training datasets: 5\n",
      "Dataset sizes: [500000, 10345, 59988, 12007, 414339]\n",
      "\n",
      "Excluding last dataset: SlimOrca_train (buggy)\n",
      "Working with 4 datasets:\n",
      "  Llama-Nemotron-Post-Training-Dataset_train: 500,000 samples\n",
      "  Bespoke-Stratos-17k_train: 10,345 samples\n",
      "  OpenThoughts-114k_train: 59,988 samples\n",
      "  databricks-dolly-15k_train: 12,007 samples\n",
      "\n",
      "Total samples from working datasets: 582,340\n",
      "\n",
      "Size-based probabilities:\n",
      "  Llama-Nemotron-Post-Training-Dataset_train: 0.859 (85.9%)\n",
      "  Bespoke-Stratos-17k_train: 0.018 (1.8%)\n",
      "  OpenThoughts-114k_train: 0.103 (10.3%)\n",
      "  databricks-dolly-15k_train: 0.021 (2.1%)\n",
      "\n",
      "Full probability array for interleave_datasets: [0.8586049386956074, 0.017764536181612115, 0.1030119861249442, 0.020618538997836316, 0.0]\n",
      "\n",
      "=== CODE FOR TRAINING SCRIPT ===\n",
      "# Replace the hard-coded probabilities with this:\n",
      "train_sizes = [len(ds[k]) for k in train_keys]\n",
      "working_sizes = train_sizes[:-1]  # Exclude last (buggy) dataset\n",
      "total_working = sum(working_sizes)\n",
      "probabilities = [size/total_working for size in working_sizes] + [0.0]\n",
      "# Result: [0.8586049386956074, 0.017764536181612115, 0.1030119861249442, 0.020618538997836316, 0.0]\n",
      "\n",
      "=== TESTING WITH NEW PROBABILITIES ===\n",
      "Successfully created interleaved dataset with 580,006 samples\n",
      "Successfully created interleaved dataset with 580,006 samples\n"
     ]
    }
   ],
   "source": [
    "# Calculate probabilities based on dataset sizes (excluding the last buggy dataset)\n",
    "print(\"=== CALCULATING PROBABILITIES BASED ON DATASET SIZES ===\")\n",
    "\n",
    "# Get dataset sizes for training sets\n",
    "train_sizes = []\n",
    "for key in train_keys:\n",
    "    size = len(ds[key])\n",
    "    train_sizes.append(size)\n",
    "    print(f\"{key}: {size:,} samples\")\n",
    "\n",
    "print(f\"\\nTotal training datasets: {len(train_keys)}\")\n",
    "print(f\"Dataset sizes: {train_sizes}\")\n",
    "\n",
    "# Exclude the last dataset (assuming it's buggy)\n",
    "if len(train_keys) > 1:\n",
    "    print(f\"\\nExcluding last dataset: {train_keys[-1]} (buggy)\")\n",
    "    working_keys = train_keys[:-1]  # All except last\n",
    "    working_sizes = train_sizes[:-1]  # All except last\n",
    "else:\n",
    "    working_keys = train_keys\n",
    "    working_sizes = train_sizes\n",
    "    print(\"\\nOnly one dataset, cannot exclude any\")\n",
    "\n",
    "print(f\"Working with {len(working_keys)} datasets:\")\n",
    "for key, size in zip(working_keys, working_sizes):\n",
    "    print(f\"  {key}: {size:,} samples\")\n",
    "\n",
    "# Calculate probabilities based on sizes\n",
    "total_working_samples = sum(working_sizes)\n",
    "size_based_probabilities = [size / total_working_samples for size in working_sizes]\n",
    "\n",
    "print(f\"\\nTotal samples from working datasets: {total_working_samples:,}\")\n",
    "print(\"\\nSize-based probabilities:\")\n",
    "for key, prob in zip(working_keys, size_based_probabilities):\n",
    "    print(f\"  {key}: {prob:.3f} ({prob*100:.1f}%)\")\n",
    "\n",
    "# Create the full probability list (including 0.0 for excluded datasets)\n",
    "full_probabilities = size_based_probabilities + [0.0] * (len(train_keys) - len(working_keys))\n",
    "print(f\"\\nFull probability array for interleave_datasets: {full_probabilities}\")\n",
    "\n",
    "print(\"\\n=== CODE FOR TRAINING SCRIPT ===\")\n",
    "print(\"# Replace the hard-coded probabilities with this:\")\n",
    "print(\"train_sizes = [len(ds[k]) for k in train_keys]\")\n",
    "print(\"working_sizes = train_sizes[:-1]  # Exclude last (buggy) dataset\") \n",
    "print(\"total_working = sum(working_sizes)\")\n",
    "print(\"probabilities = [size/total_working for size in working_sizes] + [0.0]\")\n",
    "print(f\"# Result: {full_probabilities}\")\n",
    "\n",
    "# Test the new probabilities\n",
    "print(\"\\n=== TESTING WITH NEW PROBABILITIES ===\")\n",
    "test_interleaved = interleave_datasets(train_datasets, probabilities=full_probabilities, seed=42)\n",
    "print(f\"Successfully created interleaved dataset with {len(test_interleaved):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cea44e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLEAN CODE FOR SFT_TRAIN.PY\n",
      "============================================================\n",
      "# Training dataset probabilities (size-based, excluding last):\n",
      "train_sizes = [len(ds[k]) for k in train_keys]\n",
      "working_sizes = train_sizes[:-1]\n",
      "total_working = sum(working_sizes)\n",
      "train_probabilities = [size/total_working for size in working_sizes] + [0.0]\n",
      "\n",
      "# Validation dataset probabilities (auto-balanced):\n",
      "val_probabilities = None\n",
      "\n",
      "# Create interleaved datasets:\n",
      "interleaved_train = interleave_datasets(train_datasets, probabilities=train_probabilities, seed=42)\n",
      "interleaved_val = interleave_datasets(val_datasets, probabilities=val_probabilities, seed=42)\n",
      "\n",
      "============================================================\n",
      "COPY THIS EXACT CODE BLOCK:\n",
      "============================================================\n",
      "train_sizes = [len(ds[k]) for k in train_keys]\n",
      "working_sizes = train_sizes[:-1]\n",
      "total_working = sum(working_sizes)\n",
      "train_probabilities = [size/total_working for size in working_sizes] + [0.0]\n",
      "val_probabilities = None\n",
      "interleaved_train = interleave_datasets(train_datasets, probabilities=train_probabilities, seed=42)\n",
      "interleaved_val = interleave_datasets(val_datasets, probabilities=val_probabilities, seed=42)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEAN CODE FOR SFT_TRAIN.PY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"# Training dataset probabilities (size-based, excluding last):\")\n",
    "print(\"train_sizes = [len(ds[k]) for k in train_keys]\")\n",
    "print(\"working_sizes = train_sizes[:-1]\")\n",
    "print(\"total_working = sum(working_sizes)\")\n",
    "print(\"train_probabilities = [size/total_working for size in working_sizes] + [0.0]\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"# Validation dataset probabilities (auto-balanced):\")\n",
    "print(\"val_probabilities = None\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"# Create interleaved datasets:\")\n",
    "print(\"interleaved_train = interleave_datasets(train_datasets, probabilities=train_probabilities, seed=42)\")\n",
    "print(\"interleaved_val = interleave_datasets(val_datasets, probabilities=val_probabilities, seed=42)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COPY THIS EXACT CODE BLOCK:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "code_block = \"\"\"train_sizes = [len(ds[k]) for k in train_keys]\n",
    "working_sizes = train_sizes[:-1]\n",
    "total_working = sum(working_sizes)\n",
    "train_probabilities = [size/total_working for size in working_sizes] + [0.0]\n",
    "val_probabilities = None\n",
    "interleaved_train = interleave_datasets(train_datasets, probabilities=train_probabilities, seed=42)\n",
    "interleaved_val = interleave_datasets(val_datasets, probabilities=val_probabilities, seed=42)\"\"\"\n",
    "\n",
    "print(code_block)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f989853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING SIZE-BASED PROBABILITIES ===\n",
      "Dataset sizes: [500000, 10345, 59988, 12007, 414342]\n",
      "Dataset sizes (validation): [6389973, 2594, 15025, 3003, 103590]\n",
      "Working sizes (excluding last): [500000, 10345, 59988, 12007]\n",
      "Train probabilities: [0.8586049386956074, 0.017764536181612115, 0.1030119861249442, 0.020618538997836316, 0.0]\n",
      "Val probabilities: [0.9967831379146553, 0.0004046426267764537, 0.002343776201741024, 0.0004684432568271744, 0.0]\n",
      "Total working samples: 582340\n",
      "Total working validation samples: 6410595\n",
      "\n",
      "New training dataset size: 580,006\n",
      "New validation dataset size: 6,403,591\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'collator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m train_ds_new = SFTDataset(interleaved_train_new, tokenizer, \u001b[32m8192\u001b[39m)\n\u001b[32m     41\u001b[39m val_ds_new = SFTDataset(interleaved_val_new, tokenizer, \u001b[32m8192\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m train_loader_new = DataLoader(train_ds_new, batch_size=local_batch_size, collate_fn=\u001b[43mcollator\u001b[49m)\n\u001b[32m     43\u001b[39m val_loader_new = DataLoader(val_ds_new, batch_size=local_batch_size, collate_fn=collator)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNew train loader created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_ds_new)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'collator' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import interleave_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# EXECUTABLE CODE - Test the size-based approach with dataset size limits\n",
    "print(\"=== TESTING SIZE-BASED PROBABILITIES WITH LIMITS ===\")\n",
    "local_batch_size = 8\n",
    "ds = load_from_disk(\"./sft_data/combined/\")\n",
    "\n",
    "# Dataset size limits\n",
    "TRAIN_LIMIT = 50000  # Max 50k samples per training dataset\n",
    "VAL_LIMIT = 2500     # Max 2.5k samples per validation dataset\n",
    "\n",
    "train_keys = [k for k in ds.keys() if k.endswith(\"_train\")]\n",
    "val_keys = [k for k in ds.keys() if k.endswith(\"_validation\")]\n",
    "\n",
    "# Create limited datasets\n",
    "limited_train_datasets = []\n",
    "limited_val_datasets = []\n",
    "train_sizes = []\n",
    "val_sizes = []\n",
    "\n",
    "print(\"=== TRAINING DATASETS (Limited) ===\")\n",
    "for key in train_keys:\n",
    "    original_size = len(ds[key])\n",
    "    limited_size = min(original_size, TRAIN_LIMIT)\n",
    "    limited_dataset = ds[key].select(range(limited_size))\n",
    "    limited_train_datasets.append(limited_dataset)\n",
    "    train_sizes.append(limited_size)\n",
    "    print(f\"{key}: {original_size:,} -> {limited_size:,} samples\")\n",
    "\n",
    "print(\"\\n=== VALIDATION DATASETS (Limited) ===\")\n",
    "for key in val_keys:\n",
    "    original_size = len(ds[key])\n",
    "    limited_size = min(original_size, VAL_LIMIT)\n",
    "    limited_dataset = ds[key].select(range(limited_size))\n",
    "    limited_val_datasets.append(limited_dataset)\n",
    "    val_sizes.append(limited_size)\n",
    "    print(f\"{key}: {original_size:,} -> {limited_size:,} samples\")\n",
    "\n",
    "# Calculate probabilities (excluding last training dataset)\n",
    "working_sizes = train_sizes[:-1] if len(train_sizes) > 1 else train_sizes\n",
    "working_sizes_val = val_sizes[:-1] if len(val_sizes) > 1 else val_sizes\n",
    "\n",
    "total_working = sum(working_sizes)\n",
    "total_working_val = sum(working_sizes_val) if working_sizes_val else 0\n",
    "\n",
    "train_probabilities = [size/total_working for size in working_sizes] + [0.0] if len(train_sizes) > 1 else [1.0]\n",
    "val_probabilities = [size/total_working_val for size in working_sizes_val] + [0.0] if len(val_sizes) > 1 else [1.0]\n",
    "\n",
    "print(f\"\\nTrain probabilities: {train_probabilities}\")\n",
    "print(f\"Val probabilities: {val_probabilities}\")\n",
    "\n",
    "# Create interleaved datasets\n",
    "interleaved_train_new = interleave_datasets(limited_train_datasets, probabilities=train_probabilities, seed=42)\n",
    "interleaved_val_new = interleave_datasets(limited_val_datasets, probabilities=val_probabilities, seed=42)\n",
    "\n",
    "print(f\"\\nFinal training dataset size: {len(interleaved_train_new):,}\")\n",
    "print(f\"Final validation dataset size: {len(interleaved_val_new):,}\")\n",
    "\n",
    "# Create SFT datasets and loaders\n",
    "collator = SFTDataCollator(tokenizer)\n",
    "train_ds_new = SFTDataset(interleaved_train_new, tokenizer, 8192)\n",
    "val_ds_new = SFTDataset(interleaved_val_new, tokenizer, 8192)\n",
    "train_loader_new = DataLoader(train_ds_new, batch_size=local_batch_size, collate_fn=collator)\n",
    "val_loader_new = DataLoader(val_ds_new, batch_size=local_batch_size, collate_fn=collator)\n",
    "\n",
    "print(f\"Train loader: {len(train_ds_new):,} samples\")\n",
    "print(f\"Val loader: {len(val_ds_new):,} samples\")\n",
    "\n",
    "# Test the new loader\n",
    "print(\"\\n=== TESTING LIMITED DATASET LOADER ===\")\n",
    "for batch in train_loader_new:\n",
    "    input_ids = batch['input_ids']\n",
    "    print(f\"Batch shape: {input_ids.shape}\")\n",
    "    full_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    print(f\"First example preview: {full_text[:100]}...\")\n",
    "    break\n",
    "\n",
    "print(\"\\n✅ Limited size-based probability approach working correctly!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CODE FOR SFT_TRAIN.PY WITH LIMITS:\")\n",
    "print(\"=\"*60)\n",
    "code_block = \"\"\"TRAIN_LIMIT = 50000\n",
    "VAL_LIMIT = 2500\n",
    "\n",
    "limited_train_datasets = []\n",
    "limited_val_datasets = []\n",
    "train_sizes = []\n",
    "val_sizes = []\n",
    "\n",
    "for key in train_keys:\n",
    "    limited_size = min(len(ds[key]), TRAIN_LIMIT)\n",
    "    limited_train_datasets.append(ds[key].select(range(limited_size)))\n",
    "    train_sizes.append(limited_size)\n",
    "\n",
    "for key in val_keys:\n",
    "    limited_size = min(len(ds[key]), VAL_LIMIT)\n",
    "    limited_val_datasets.append(ds[key].select(range(limited_size)))\n",
    "    val_sizes.append(limited_size)\n",
    "\n",
    "working_sizes = train_sizes[:-1] if len(train_sizes) > 1 else train_sizes\n",
    "working_sizes_val = val_sizes[:-1] if len(val_sizes) > 1 else val_sizes\n",
    "total_working = sum(working_sizes)\n",
    "total_working_val = sum(working_sizes_val) if working_sizes_val else 0\n",
    "\n",
    "train_probabilities = [size/total_working for size in working_sizes] + [0.0] if len(train_sizes) > 1 else [1.0]\n",
    "val_probabilities = [size/total_working_val for size in working_sizes_val] + [0.0] if len(val_sizes) > 1 else [1.0]\n",
    "\n",
    "interleaved_train = interleave_datasets(limited_train_datasets, probabilities=train_probabilities, seed=42)\n",
    "interleaved_val = interleave_datasets(limited_val_datasets, probabilities=val_probabilities, seed=42)\"\"\"\n",
    "\n",
    "print(code_block)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70460c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
